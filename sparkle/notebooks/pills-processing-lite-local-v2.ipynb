{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "toc": true
   },
   "source": [
    "<h1>Table of Contents<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#Imports-&amp;-Env-Setup\" data-toc-modified-id=\"Imports-&amp;-Env-Setup-1\">Imports &amp; Env Setup</a></span></li><li><span><a href=\"#Grab-Data-From-s3-Using-Collin's-Functions\" data-toc-modified-id=\"Grab-Data-From-s3-Using-Collin's-Functions-2\">Grab Data From s3 Using Collin's Functions</a></span></li><li><span><a href=\"#Kevin's-Pre-processing-Functions\" data-toc-modified-id=\"Kevin's-Pre-processing-Functions-3\">Kevin's Pre-processing Functions</a></span></li><li><span><a href=\"#Edited-Pre-processing-Functions\" data-toc-modified-id=\"Edited-Pre-processing-Functions-4\">Edited Pre-processing Functions</a></span></li><li><span><a href=\"#Generate-DFs-in-Mini-batches\" data-toc-modified-id=\"Generate-DFs-in-Mini-batches-5\">Generate DFs in Mini-batches</a></span></li><li><span><a href=\"#Transform-Data\" data-toc-modified-id=\"Transform-Data-6\">Transform Data</a></span></li><li><span><a href=\"#Write-Mini-DFs-to-Disk\" data-toc-modified-id=\"Write-Mini-DFs-to-Disk-7\">Write Mini-DFs to Disk</a></span></li><li><span><a href=\"#Join-Mini-DFs-to-Get-BigDF\" data-toc-modified-id=\"Join-Mini-DFs-to-Get-BigDF-8\">Join Mini-DFs to Get BigDF</a></span></li><li><span><a href=\"#Training\" data-toc-modified-id=\"Training-9\">Training</a></span></li></ul></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Imports & Env Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-31T23:30:24.798432Z",
     "start_time": "2020-01-31T23:30:24.487535Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import math\n",
    "import numpy as np\n",
    "\n",
    "from pyspark import SparkContext\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql.functions import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-31T23:30:24.824765Z",
     "start_time": "2020-01-31T23:30:24.800985Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Configure pyspark env\n",
    "os.environ['PYSPARK_SUBMIT_ARGS'] = '--packages \"org.apache.hadoop:hadoop-aws:2.7.4\" pyspark-shell'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-31T23:30:30.945108Z",
     "start_time": "2020-01-31T23:30:24.830640Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# SparkContet must come AFTER env set-up!\n",
    "sc = SparkContext.getOrCreate()\n",
    "ss = SparkSession.builder.getOrCreate()\n",
    "\n",
    "sc._jsc.hadoopConfiguration().set(\"fs.s3.impl\", \"org.apache.hadoop.fs.s3a.S3AFileSystem\")\n",
    "sc._jsc.hadoopConfiguration().set('fs.s3a.access.key', 'AKIAW7CYB6L5UCPV525G')    # Access Key\n",
    "sc._jsc.hadoopConfiguration().set('fs.s3a.secret.key', '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') # Secret Key"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "-----------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Grab Data From s3 Using Collin's Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-31T23:30:31.517643Z",
     "start_time": "2020-01-31T23:30:30.948225Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "import boto3\n",
    "from collections import defaultdict\n",
    "\n",
    "# here for testing purposes\n",
    "session = boto3.session.Session(profile_name='sparkle')\n",
    "s3_client = session.client('s3')\n",
    "\n",
    "def get_s3_page_iterator(profile='sparkle', bucket='msds-sparkle', prefix='data/output/pills_all_data/'):\n",
    "    \"\"\"\n",
    "    Assuming aws keys are stored in specified profile,\n",
    "    this fn retrieves a paginator (iterable) in the specified s3 location,\n",
    "    note: pagination approach solves problem of maxing out at 1000 csv's\n",
    "          https://adamj.eu/tech/2018/01/09/using-boto3-think-pagination/\n",
    "    note: w/o a prefix, it can recursively reach all paths! Though,\n",
    "          this would break our `npills` splitting at bottom of `get_paths`\n",
    "          function, though this could be easily adjusted.\n",
    "    \"\"\"\n",
    "    session = boto3.session.Session(profile_name=profile)\n",
    "    s3_client = session.client('s3')\n",
    "    paginator = s3_client.get_paginator('list_objects')\n",
    "    page_iterator = paginator.paginate(Bucket=bucket, Prefix=prefix)\n",
    "    return page_iterator\n",
    "\n",
    "def get_paths(page_iterator):\n",
    "    \"\"\"\n",
    "    Given an s3 paginator, this fn retrieves the path to each file in\n",
    "    the s3 location, then returns them as a dict with each key being the # of pills\n",
    "    that data observed and each value being a list of s3_paths.\n",
    "    \"\"\"\n",
    "    s3_paths = defaultdict(list)\n",
    "    for page in page_iterator:\n",
    "        for obj in page['Contents']:\n",
    "            path = obj[\"Key\"]\n",
    "            # Split on number of slashes in prefix\n",
    "            npills = path.split(\"/\")[2].split(\"-\")[2]\n",
    "            s3_paths[int(npills)].append(path)\n",
    "    return s3_paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-31T23:30:32.283900Z",
     "start_time": "2020-01-31T23:30:31.521856Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Grab file paths from s3\n",
    "\n",
    "bucket = 'msds-twinkle'\n",
    "prefix = 'data/processed/'\n",
    "\n",
    "s3_iter = get_s3_page_iterator(bucket=bucket, prefix=prefix)\n",
    "s3_paths = get_paths(s3_iter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-31T23:30:32.299701Z",
     "start_time": "2020-01-31T23:30:32.286075Z"
    },
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['data/processed/pills-01-00-01.csv',\n",
       "  'data/processed/pills-01-00-02.csv',\n",
       "  'data/processed/pills-01-00-03.csv',\n",
       "  'data/processed/pills-01-00-04.csv',\n",
       "  'data/processed/pills-01-00-05.csv',\n",
       "  'data/processed/pills-02-00-01.csv',\n",
       "  'data/processed/pills-02-00-02.csv',\n",
       "  'data/processed/pills-02-00-03.csv',\n",
       "  'data/processed/pills-02-00-04.csv',\n",
       "  'data/processed/pills-02-00-05.csv',\n",
       "  'data/processed/pills-03-00-01.csv',\n",
       "  'data/processed/pills-03-00-02.csv',\n",
       "  'data/processed/pills-03-00-03.csv',\n",
       "  'data/processed/pills-03-00-04.csv',\n",
       "  'data/processed/pills-03-00-05.csv',\n",
       "  'data/processed/pills-04-00-01.csv',\n",
       "  'data/processed/pills-04-00-02.csv',\n",
       "  'data/processed/pills-04-00-03.csv',\n",
       "  'data/processed/pills-04-00-04.csv',\n",
       "  'data/processed/pills-04-00-05.csv',\n",
       "  'data/processed/pills-05-00-01.csv',\n",
       "  'data/processed/pills-05-00-02.csv',\n",
       "  'data/processed/pills-05-00-03.csv',\n",
       "  'data/processed/pills-05-00-04.csv',\n",
       "  'data/processed/pills-05-00-05.csv',\n",
       "  'data/processed/pills-06-00-01.csv',\n",
       "  'data/processed/pills-06-00-02.csv',\n",
       "  'data/processed/pills-06-00-03.csv',\n",
       "  'data/processed/pills-06-00-04.csv',\n",
       "  'data/processed/pills-06-00-05.csv',\n",
       "  'data/processed/pills-07-00-01.csv',\n",
       "  'data/processed/pills-07-00-02.csv',\n",
       "  'data/processed/pills-07-00-03.csv',\n",
       "  'data/processed/pills-07-00-04.csv',\n",
       "  'data/processed/pills-07-00-05.csv',\n",
       "  'data/processed/pills-08-00-01.csv',\n",
       "  'data/processed/pills-08-00-02.csv',\n",
       "  'data/processed/pills-08-00-03.csv',\n",
       "  'data/processed/pills-08-00-04.csv',\n",
       "  'data/processed/pills-08-00-05.csv',\n",
       "  'data/processed/pills-09-00-01.csv',\n",
       "  'data/processed/pills-09-00-02.csv',\n",
       "  'data/processed/pills-09-00-03.csv',\n",
       "  'data/processed/pills-09-00-04.csv',\n",
       "  'data/processed/pills-09-00-05.csv',\n",
       "  'data/processed/pills-10-00-01.csv',\n",
       "  'data/processed/pills-10-00-02.csv',\n",
       "  'data/processed/pills-10-00-03.csv',\n",
       "  'data/processed/pills-10-00-04.csv',\n",
       "  'data/processed/pills-10-00-05.csv',\n",
       "  'data/processed/pills-11-00-01.csv',\n",
       "  'data/processed/pills-11-00-02.csv',\n",
       "  'data/processed/pills-11-00-03.csv',\n",
       "  'data/processed/pills-11-00-04.csv',\n",
       "  'data/processed/pills-11-00-05.csv',\n",
       "  'data/processed/pills-12-00-01.csv',\n",
       "  'data/processed/pills-12-00-02.csv',\n",
       "  'data/processed/pills-12-00-03.csv',\n",
       "  'data/processed/pills-12-00-04.csv',\n",
       "  'data/processed/pills-12-00-05.csv',\n",
       "  'data/processed/pills-13-00-01.csv',\n",
       "  'data/processed/pills-13-00-02.csv',\n",
       "  'data/processed/pills-13-00-03.csv',\n",
       "  'data/processed/pills-13-00-04.csv',\n",
       "  'data/processed/pills-13-00-05.csv',\n",
       "  'data/processed/pills-14-00-01.csv',\n",
       "  'data/processed/pills-14-00-02.csv',\n",
       "  'data/processed/pills-14-00-03.csv',\n",
       "  'data/processed/pills-14-00-04.csv',\n",
       "  'data/processed/pills-14-00-05.csv',\n",
       "  'data/processed/pills-15-00-01.csv',\n",
       "  'data/processed/pills-15-00-02.csv',\n",
       "  'data/processed/pills-15-00-03.csv',\n",
       "  'data/processed/pills-15-00-04.csv',\n",
       "  'data/processed/pills-15-00-05.csv',\n",
       "  'data/processed/pills-16-00-01.csv',\n",
       "  'data/processed/pills-16-00-02.csv',\n",
       "  'data/processed/pills-16-00-03.csv',\n",
       "  'data/processed/pills-16-00-04.csv',\n",
       "  'data/processed/pills-16-00-05.csv',\n",
       "  'data/processed/pills-17-00-01.csv',\n",
       "  'data/processed/pills-17-00-02.csv',\n",
       "  'data/processed/pills-17-00-03.csv',\n",
       "  'data/processed/pills-17-00-04.csv',\n",
       "  'data/processed/pills-17-00-05.csv'],\n",
       " ['data/processed/pills-01-01-01.csv',\n",
       "  'data/processed/pills-01-01-02.csv',\n",
       "  'data/processed/pills-01-01-03.csv',\n",
       "  'data/processed/pills-01-01-04.csv',\n",
       "  'data/processed/pills-01-01-05.csv',\n",
       "  'data/processed/pills-02-01-01.csv',\n",
       "  'data/processed/pills-02-01-02.csv',\n",
       "  'data/processed/pills-02-01-03.csv',\n",
       "  'data/processed/pills-02-01-04.csv',\n",
       "  'data/processed/pills-02-01-05.csv',\n",
       "  'data/processed/pills-03-01-01.csv',\n",
       "  'data/processed/pills-03-01-02.csv',\n",
       "  'data/processed/pills-03-01-03.csv',\n",
       "  'data/processed/pills-03-01-04.csv',\n",
       "  'data/processed/pills-03-01-05.csv',\n",
       "  'data/processed/pills-04-01-01.csv',\n",
       "  'data/processed/pills-04-01-02.csv',\n",
       "  'data/processed/pills-04-01-03.csv',\n",
       "  'data/processed/pills-04-01-04.csv',\n",
       "  'data/processed/pills-04-01-05.csv',\n",
       "  'data/processed/pills-05-01-01.csv',\n",
       "  'data/processed/pills-05-01-02.csv',\n",
       "  'data/processed/pills-05-01-03.csv',\n",
       "  'data/processed/pills-05-01-04.csv',\n",
       "  'data/processed/pills-05-01-05.csv',\n",
       "  'data/processed/pills-06-01-01.csv',\n",
       "  'data/processed/pills-06-01-02.csv',\n",
       "  'data/processed/pills-06-01-03.csv',\n",
       "  'data/processed/pills-06-01-04.csv',\n",
       "  'data/processed/pills-06-01-05.csv',\n",
       "  'data/processed/pills-07-01-01.csv',\n",
       "  'data/processed/pills-07-01-02.csv',\n",
       "  'data/processed/pills-07-01-03.csv',\n",
       "  'data/processed/pills-07-01-04.csv',\n",
       "  'data/processed/pills-07-01-05.csv',\n",
       "  'data/processed/pills-08-01-01.csv',\n",
       "  'data/processed/pills-08-01-02.csv',\n",
       "  'data/processed/pills-08-01-03.csv',\n",
       "  'data/processed/pills-08-01-04.csv',\n",
       "  'data/processed/pills-08-01-05.csv',\n",
       "  'data/processed/pills-09-01-01.csv',\n",
       "  'data/processed/pills-09-01-02.csv',\n",
       "  'data/processed/pills-09-01-03.csv',\n",
       "  'data/processed/pills-09-01-04.csv',\n",
       "  'data/processed/pills-09-01-05.csv',\n",
       "  'data/processed/pills-10-01-01.csv',\n",
       "  'data/processed/pills-10-01-02.csv',\n",
       "  'data/processed/pills-10-01-03.csv',\n",
       "  'data/processed/pills-10-01-04.csv',\n",
       "  'data/processed/pills-10-01-05.csv',\n",
       "  'data/processed/pills-11-01-01.csv',\n",
       "  'data/processed/pills-11-01-02.csv',\n",
       "  'data/processed/pills-11-01-03.csv',\n",
       "  'data/processed/pills-11-01-04.csv',\n",
       "  'data/processed/pills-11-01-05.csv',\n",
       "  'data/processed/pills-12-01-01.csv',\n",
       "  'data/processed/pills-12-01-02.csv',\n",
       "  'data/processed/pills-12-01-03.csv',\n",
       "  'data/processed/pills-12-01-04.csv',\n",
       "  'data/processed/pills-12-01-05.csv',\n",
       "  'data/processed/pills-13-01-01.csv',\n",
       "  'data/processed/pills-13-01-02.csv',\n",
       "  'data/processed/pills-13-01-03.csv',\n",
       "  'data/processed/pills-13-01-04.csv',\n",
       "  'data/processed/pills-13-01-05.csv',\n",
       "  'data/processed/pills-14-01-01.csv',\n",
       "  'data/processed/pills-14-01-02.csv',\n",
       "  'data/processed/pills-14-01-03.csv',\n",
       "  'data/processed/pills-14-01-04.csv',\n",
       "  'data/processed/pills-14-01-05.csv',\n",
       "  'data/processed/pills-15-01-01.csv',\n",
       "  'data/processed/pills-15-01-02.csv',\n",
       "  'data/processed/pills-15-01-03.csv',\n",
       "  'data/processed/pills-15-01-04.csv',\n",
       "  'data/processed/pills-15-01-05.csv',\n",
       "  'data/processed/pills-16-01-01.csv',\n",
       "  'data/processed/pills-16-01-02.csv',\n",
       "  'data/processed/pills-16-01-03.csv',\n",
       "  'data/processed/pills-16-01-04.csv',\n",
       "  'data/processed/pills-16-01-05.csv',\n",
       "  'data/processed/pills-17-01-01.csv',\n",
       "  'data/processed/pills-17-01-02.csv',\n",
       "  'data/processed/pills-17-01-03.csv',\n",
       "  'data/processed/pills-17-01-04.csv',\n",
       "  'data/processed/pills-17-01-05.csv'],\n",
       " ['data/processed/pills-01-05-01.csv',\n",
       "  'data/processed/pills-01-05-02.csv',\n",
       "  'data/processed/pills-01-05-03.csv',\n",
       "  'data/processed/pills-01-05-04.csv',\n",
       "  'data/processed/pills-01-05-05.csv',\n",
       "  'data/processed/pills-02-05-01.csv',\n",
       "  'data/processed/pills-02-05-02.csv',\n",
       "  'data/processed/pills-02-05-03.csv',\n",
       "  'data/processed/pills-02-05-04.csv',\n",
       "  'data/processed/pills-02-05-05.csv',\n",
       "  'data/processed/pills-03-05-01.csv',\n",
       "  'data/processed/pills-03-05-02.csv',\n",
       "  'data/processed/pills-03-05-03.csv',\n",
       "  'data/processed/pills-03-05-04.csv',\n",
       "  'data/processed/pills-03-05-05.csv',\n",
       "  'data/processed/pills-04-05-01.csv',\n",
       "  'data/processed/pills-04-05-02.csv',\n",
       "  'data/processed/pills-04-05-03.csv',\n",
       "  'data/processed/pills-04-05-04.csv',\n",
       "  'data/processed/pills-04-05-05.csv',\n",
       "  'data/processed/pills-05-05-01.csv',\n",
       "  'data/processed/pills-05-05-02.csv',\n",
       "  'data/processed/pills-05-05-03.csv',\n",
       "  'data/processed/pills-05-05-04.csv',\n",
       "  'data/processed/pills-05-05-05.csv',\n",
       "  'data/processed/pills-06-05-01.csv',\n",
       "  'data/processed/pills-06-05-02.csv',\n",
       "  'data/processed/pills-06-05-03.csv',\n",
       "  'data/processed/pills-06-05-04.csv',\n",
       "  'data/processed/pills-06-05-05.csv',\n",
       "  'data/processed/pills-07-05-01.csv',\n",
       "  'data/processed/pills-07-05-02.csv',\n",
       "  'data/processed/pills-07-05-03.csv',\n",
       "  'data/processed/pills-07-05-04.csv',\n",
       "  'data/processed/pills-07-05-05.csv',\n",
       "  'data/processed/pills-08-05-01.csv',\n",
       "  'data/processed/pills-08-05-02.csv',\n",
       "  'data/processed/pills-08-05-03.csv',\n",
       "  'data/processed/pills-08-05-04.csv',\n",
       "  'data/processed/pills-08-05-05.csv',\n",
       "  'data/processed/pills-09-05-01.csv',\n",
       "  'data/processed/pills-09-05-02.csv',\n",
       "  'data/processed/pills-09-05-03.csv',\n",
       "  'data/processed/pills-09-05-04.csv',\n",
       "  'data/processed/pills-09-05-05.csv',\n",
       "  'data/processed/pills-10-05-01.csv',\n",
       "  'data/processed/pills-10-05-02.csv',\n",
       "  'data/processed/pills-10-05-03.csv',\n",
       "  'data/processed/pills-10-05-04.csv',\n",
       "  'data/processed/pills-10-05-05.csv',\n",
       "  'data/processed/pills-11-05-01.csv',\n",
       "  'data/processed/pills-11-05-02.csv',\n",
       "  'data/processed/pills-11-05-03.csv',\n",
       "  'data/processed/pills-11-05-04.csv',\n",
       "  'data/processed/pills-11-05-05.csv',\n",
       "  'data/processed/pills-12-05-01.csv',\n",
       "  'data/processed/pills-12-05-02.csv',\n",
       "  'data/processed/pills-12-05-03.csv',\n",
       "  'data/processed/pills-12-05-04.csv',\n",
       "  'data/processed/pills-12-05-05.csv',\n",
       "  'data/processed/pills-13-05-01.csv',\n",
       "  'data/processed/pills-13-05-02.csv',\n",
       "  'data/processed/pills-13-05-03.csv',\n",
       "  'data/processed/pills-13-05-04.csv',\n",
       "  'data/processed/pills-13-05-05.csv',\n",
       "  'data/processed/pills-14-05-01.csv',\n",
       "  'data/processed/pills-14-05-02.csv',\n",
       "  'data/processed/pills-14-05-03.csv',\n",
       "  'data/processed/pills-14-05-04.csv',\n",
       "  'data/processed/pills-14-05-05.csv',\n",
       "  'data/processed/pills-15-05-01.csv',\n",
       "  'data/processed/pills-15-05-02.csv',\n",
       "  'data/processed/pills-15-05-03.csv',\n",
       "  'data/processed/pills-15-05-04.csv',\n",
       "  'data/processed/pills-15-05-05.csv',\n",
       "  'data/processed/pills-16-05-01.csv',\n",
       "  'data/processed/pills-16-05-02.csv',\n",
       "  'data/processed/pills-16-05-03.csv',\n",
       "  'data/processed/pills-16-05-04.csv',\n",
       "  'data/processed/pills-16-05-05.csv',\n",
       "  'data/processed/pills-17-05-01.csv',\n",
       "  'data/processed/pills-17-05-02.csv',\n",
       "  'data/processed/pills-17-05-03.csv',\n",
       "  'data/processed/pills-17-05-04.csv',\n",
       "  'data/processed/pills-17-05-05.csv'],\n",
       " ['data/processed/pills-01-10-01.csv',\n",
       "  'data/processed/pills-01-10-02.csv',\n",
       "  'data/processed/pills-01-10-03.csv',\n",
       "  'data/processed/pills-01-10-04.csv',\n",
       "  'data/processed/pills-01-10-05.csv',\n",
       "  'data/processed/pills-02-10-01.csv',\n",
       "  'data/processed/pills-02-10-02.csv',\n",
       "  'data/processed/pills-02-10-03.csv',\n",
       "  'data/processed/pills-02-10-04.csv',\n",
       "  'data/processed/pills-02-10-05.csv',\n",
       "  'data/processed/pills-03-10-01.csv',\n",
       "  'data/processed/pills-03-10-02.csv',\n",
       "  'data/processed/pills-03-10-03.csv',\n",
       "  'data/processed/pills-03-10-04.csv',\n",
       "  'data/processed/pills-03-10-05.csv',\n",
       "  'data/processed/pills-04-10-01.csv',\n",
       "  'data/processed/pills-04-10-02.csv',\n",
       "  'data/processed/pills-04-10-03.csv',\n",
       "  'data/processed/pills-04-10-04.csv',\n",
       "  'data/processed/pills-04-10-05.csv',\n",
       "  'data/processed/pills-05-10-01.csv',\n",
       "  'data/processed/pills-05-10-02.csv',\n",
       "  'data/processed/pills-05-10-03.csv',\n",
       "  'data/processed/pills-05-10-04.csv',\n",
       "  'data/processed/pills-05-10-05.csv',\n",
       "  'data/processed/pills-06-10-01.csv',\n",
       "  'data/processed/pills-06-10-02.csv',\n",
       "  'data/processed/pills-06-10-03.csv',\n",
       "  'data/processed/pills-06-10-04.csv',\n",
       "  'data/processed/pills-06-10-05.csv',\n",
       "  'data/processed/pills-07-10-01.csv',\n",
       "  'data/processed/pills-07-10-02.csv',\n",
       "  'data/processed/pills-07-10-03.csv',\n",
       "  'data/processed/pills-07-10-04.csv',\n",
       "  'data/processed/pills-07-10-05.csv',\n",
       "  'data/processed/pills-08-10-01.csv',\n",
       "  'data/processed/pills-08-10-02.csv',\n",
       "  'data/processed/pills-08-10-03.csv',\n",
       "  'data/processed/pills-08-10-04.csv',\n",
       "  'data/processed/pills-08-10-05.csv',\n",
       "  'data/processed/pills-09-10-01.csv',\n",
       "  'data/processed/pills-09-10-02.csv',\n",
       "  'data/processed/pills-09-10-03.csv',\n",
       "  'data/processed/pills-09-10-04.csv',\n",
       "  'data/processed/pills-09-10-05.csv',\n",
       "  'data/processed/pills-10-10-01.csv',\n",
       "  'data/processed/pills-10-10-02.csv',\n",
       "  'data/processed/pills-10-10-03.csv',\n",
       "  'data/processed/pills-10-10-04.csv',\n",
       "  'data/processed/pills-10-10-05.csv',\n",
       "  'data/processed/pills-11-10-01.csv',\n",
       "  'data/processed/pills-11-10-02.csv',\n",
       "  'data/processed/pills-11-10-03.csv',\n",
       "  'data/processed/pills-11-10-04.csv',\n",
       "  'data/processed/pills-11-10-05.csv',\n",
       "  'data/processed/pills-12-10-01.csv',\n",
       "  'data/processed/pills-12-10-02.csv',\n",
       "  'data/processed/pills-12-10-03.csv',\n",
       "  'data/processed/pills-12-10-04.csv',\n",
       "  'data/processed/pills-12-10-05.csv',\n",
       "  'data/processed/pills-13-10-01.csv',\n",
       "  'data/processed/pills-13-10-02.csv',\n",
       "  'data/processed/pills-13-10-03.csv',\n",
       "  'data/processed/pills-13-10-04.csv',\n",
       "  'data/processed/pills-13-10-05.csv',\n",
       "  'data/processed/pills-14-10-01.csv',\n",
       "  'data/processed/pills-14-10-02.csv',\n",
       "  'data/processed/pills-14-10-03.csv',\n",
       "  'data/processed/pills-14-10-04.csv',\n",
       "  'data/processed/pills-14-10-05.csv',\n",
       "  'data/processed/pills-15-10-01.csv',\n",
       "  'data/processed/pills-15-10-02.csv',\n",
       "  'data/processed/pills-15-10-03.csv',\n",
       "  'data/processed/pills-15-10-04.csv',\n",
       "  'data/processed/pills-15-10-05.csv',\n",
       "  'data/processed/pills-16-10-01.csv',\n",
       "  'data/processed/pills-16-10-02.csv',\n",
       "  'data/processed/pills-16-10-03.csv',\n",
       "  'data/processed/pills-16-10-04.csv',\n",
       "  'data/processed/pills-16-10-05.csv',\n",
       "  'data/processed/pills-17-10-01.csv',\n",
       "  'data/processed/pills-17-10-02.csv',\n",
       "  'data/processed/pills-17-10-03.csv',\n",
       "  'data/processed/pills-17-10-04.csv',\n",
       "  'data/processed/pills-17-10-05.csv'],\n",
       " ['data/processed/pills-01-15-01.csv',\n",
       "  'data/processed/pills-01-15-02.csv',\n",
       "  'data/processed/pills-01-15-03.csv',\n",
       "  'data/processed/pills-01-15-04.csv',\n",
       "  'data/processed/pills-01-15-05.csv',\n",
       "  'data/processed/pills-02-15-01.csv',\n",
       "  'data/processed/pills-02-15-02.csv',\n",
       "  'data/processed/pills-02-15-03.csv',\n",
       "  'data/processed/pills-02-15-04.csv',\n",
       "  'data/processed/pills-02-15-05.csv',\n",
       "  'data/processed/pills-03-15-01.csv',\n",
       "  'data/processed/pills-03-15-02.csv',\n",
       "  'data/processed/pills-03-15-03.csv',\n",
       "  'data/processed/pills-03-15-04.csv',\n",
       "  'data/processed/pills-03-15-05.csv',\n",
       "  'data/processed/pills-04-15-01.csv',\n",
       "  'data/processed/pills-04-15-02.csv',\n",
       "  'data/processed/pills-04-15-03.csv',\n",
       "  'data/processed/pills-04-15-04.csv',\n",
       "  'data/processed/pills-04-15-05.csv',\n",
       "  'data/processed/pills-05-15-01.csv',\n",
       "  'data/processed/pills-05-15-02.csv',\n",
       "  'data/processed/pills-05-15-03.csv',\n",
       "  'data/processed/pills-05-15-04.csv',\n",
       "  'data/processed/pills-05-15-05.csv',\n",
       "  'data/processed/pills-06-15-01.csv',\n",
       "  'data/processed/pills-06-15-02.csv',\n",
       "  'data/processed/pills-06-15-03.csv',\n",
       "  'data/processed/pills-06-15-04.csv',\n",
       "  'data/processed/pills-06-15-05.csv',\n",
       "  'data/processed/pills-07-15-01.csv',\n",
       "  'data/processed/pills-07-15-02.csv',\n",
       "  'data/processed/pills-07-15-03.csv',\n",
       "  'data/processed/pills-07-15-04.csv',\n",
       "  'data/processed/pills-07-15-05.csv',\n",
       "  'data/processed/pills-08-15-01.csv',\n",
       "  'data/processed/pills-08-15-02.csv',\n",
       "  'data/processed/pills-08-15-03.csv',\n",
       "  'data/processed/pills-08-15-04.csv',\n",
       "  'data/processed/pills-08-15-05.csv',\n",
       "  'data/processed/pills-09-15-01.csv',\n",
       "  'data/processed/pills-09-15-02.csv',\n",
       "  'data/processed/pills-09-15-03.csv',\n",
       "  'data/processed/pills-09-15-04.csv',\n",
       "  'data/processed/pills-09-15-05.csv',\n",
       "  'data/processed/pills-10-15-01.csv',\n",
       "  'data/processed/pills-10-15-02.csv',\n",
       "  'data/processed/pills-10-15-03.csv',\n",
       "  'data/processed/pills-10-15-04.csv',\n",
       "  'data/processed/pills-10-15-05.csv',\n",
       "  'data/processed/pills-11-15-01.csv',\n",
       "  'data/processed/pills-11-15-02.csv',\n",
       "  'data/processed/pills-11-15-03.csv',\n",
       "  'data/processed/pills-11-15-04.csv',\n",
       "  'data/processed/pills-11-15-05.csv',\n",
       "  'data/processed/pills-12-15-01.csv',\n",
       "  'data/processed/pills-12-15-02.csv',\n",
       "  'data/processed/pills-12-15-03.csv',\n",
       "  'data/processed/pills-12-15-04.csv',\n",
       "  'data/processed/pills-12-15-05.csv',\n",
       "  'data/processed/pills-13-15-01.csv',\n",
       "  'data/processed/pills-13-15-02.csv',\n",
       "  'data/processed/pills-13-15-03.csv',\n",
       "  'data/processed/pills-13-15-04.csv',\n",
       "  'data/processed/pills-13-15-05.csv',\n",
       "  'data/processed/pills-14-15-01.csv',\n",
       "  'data/processed/pills-14-15-02.csv',\n",
       "  'data/processed/pills-14-15-03.csv',\n",
       "  'data/processed/pills-14-15-04.csv',\n",
       "  'data/processed/pills-14-15-05.csv',\n",
       "  'data/processed/pills-15-15-01.csv',\n",
       "  'data/processed/pills-15-15-02.csv',\n",
       "  'data/processed/pills-15-15-03.csv',\n",
       "  'data/processed/pills-15-15-04.csv',\n",
       "  'data/processed/pills-15-15-05.csv',\n",
       "  'data/processed/pills-16-15-01.csv',\n",
       "  'data/processed/pills-16-15-02.csv',\n",
       "  'data/processed/pills-16-15-03.csv',\n",
       "  'data/processed/pills-16-15-04.csv',\n",
       "  'data/processed/pills-16-15-05.csv',\n",
       "  'data/processed/pills-17-15-01.csv',\n",
       "  'data/processed/pills-17-15-02.csv',\n",
       "  'data/processed/pills-17-15-03.csv',\n",
       "  'data/processed/pills-17-15-04.csv',\n",
       "  'data/processed/pills-17-15-05.csv'],\n",
       " ['data/processed/pills-01-20-01.csv',\n",
       "  'data/processed/pills-01-20-02.csv',\n",
       "  'data/processed/pills-01-20-03.csv',\n",
       "  'data/processed/pills-01-20-04.csv',\n",
       "  'data/processed/pills-01-20-05.csv',\n",
       "  'data/processed/pills-02-20-01.csv',\n",
       "  'data/processed/pills-02-20-02.csv',\n",
       "  'data/processed/pills-02-20-03.csv',\n",
       "  'data/processed/pills-02-20-04.csv',\n",
       "  'data/processed/pills-02-20-05.csv',\n",
       "  'data/processed/pills-03-20-01.csv',\n",
       "  'data/processed/pills-03-20-02.csv',\n",
       "  'data/processed/pills-03-20-03.csv',\n",
       "  'data/processed/pills-03-20-04.csv',\n",
       "  'data/processed/pills-03-20-05.csv',\n",
       "  'data/processed/pills-04-20-01.csv',\n",
       "  'data/processed/pills-04-20-02.csv',\n",
       "  'data/processed/pills-04-20-03.csv',\n",
       "  'data/processed/pills-04-20-04.csv',\n",
       "  'data/processed/pills-04-20-05.csv',\n",
       "  'data/processed/pills-05-20-01.csv',\n",
       "  'data/processed/pills-05-20-02.csv',\n",
       "  'data/processed/pills-05-20-03.csv',\n",
       "  'data/processed/pills-05-20-04.csv',\n",
       "  'data/processed/pills-05-20-05.csv',\n",
       "  'data/processed/pills-06-20-01.csv',\n",
       "  'data/processed/pills-06-20-02.csv',\n",
       "  'data/processed/pills-06-20-03.csv',\n",
       "  'data/processed/pills-06-20-04.csv',\n",
       "  'data/processed/pills-06-20-05.csv',\n",
       "  'data/processed/pills-07-20-01.csv',\n",
       "  'data/processed/pills-07-20-02.csv',\n",
       "  'data/processed/pills-07-20-03.csv',\n",
       "  'data/processed/pills-07-20-04.csv',\n",
       "  'data/processed/pills-07-20-05.csv',\n",
       "  'data/processed/pills-08-20-01.csv',\n",
       "  'data/processed/pills-08-20-02.csv',\n",
       "  'data/processed/pills-08-20-03.csv',\n",
       "  'data/processed/pills-08-20-04.csv',\n",
       "  'data/processed/pills-08-20-05.csv',\n",
       "  'data/processed/pills-09-20-01.csv',\n",
       "  'data/processed/pills-09-20-02.csv',\n",
       "  'data/processed/pills-09-20-03.csv',\n",
       "  'data/processed/pills-09-20-04.csv',\n",
       "  'data/processed/pills-09-20-05.csv',\n",
       "  'data/processed/pills-10-20-01.csv',\n",
       "  'data/processed/pills-10-20-02.csv',\n",
       "  'data/processed/pills-10-20-03.csv',\n",
       "  'data/processed/pills-10-20-04.csv',\n",
       "  'data/processed/pills-10-20-05.csv',\n",
       "  'data/processed/pills-11-20-01.csv',\n",
       "  'data/processed/pills-11-20-02.csv',\n",
       "  'data/processed/pills-11-20-03.csv',\n",
       "  'data/processed/pills-11-20-04.csv',\n",
       "  'data/processed/pills-11-20-05.csv',\n",
       "  'data/processed/pills-12-20-01.csv',\n",
       "  'data/processed/pills-12-20-02.csv',\n",
       "  'data/processed/pills-12-20-03.csv',\n",
       "  'data/processed/pills-12-20-04.csv',\n",
       "  'data/processed/pills-12-20-05.csv',\n",
       "  'data/processed/pills-13-20-01.csv',\n",
       "  'data/processed/pills-13-20-02.csv',\n",
       "  'data/processed/pills-13-20-03.csv',\n",
       "  'data/processed/pills-13-20-04.csv',\n",
       "  'data/processed/pills-13-20-05.csv',\n",
       "  'data/processed/pills-14-20-01.csv',\n",
       "  'data/processed/pills-14-20-02.csv',\n",
       "  'data/processed/pills-14-20-03.csv',\n",
       "  'data/processed/pills-14-20-04.csv',\n",
       "  'data/processed/pills-14-20-05.csv',\n",
       "  'data/processed/pills-15-20-01.csv',\n",
       "  'data/processed/pills-15-20-02.csv',\n",
       "  'data/processed/pills-15-20-03.csv',\n",
       "  'data/processed/pills-15-20-04.csv',\n",
       "  'data/processed/pills-15-20-05.csv',\n",
       "  'data/processed/pills-16-20-01.csv',\n",
       "  'data/processed/pills-16-20-02.csv',\n",
       "  'data/processed/pills-16-20-03.csv',\n",
       "  'data/processed/pills-16-20-04.csv',\n",
       "  'data/processed/pills-16-20-05.csv',\n",
       "  'data/processed/pills-17-20-01.csv',\n",
       "  'data/processed/pills-17-20-02.csv',\n",
       "  'data/processed/pills-17-20-03.csv',\n",
       "  'data/processed/pills-17-20-04.csv',\n",
       "  'data/processed/pills-17-20-05.csv'],\n",
       " ['data/processed/pills-01-25-01.csv',\n",
       "  'data/processed/pills-01-25-02.csv',\n",
       "  'data/processed/pills-01-25-03.csv',\n",
       "  'data/processed/pills-01-25-04.csv',\n",
       "  'data/processed/pills-01-25-05.csv',\n",
       "  'data/processed/pills-02-25-01.csv',\n",
       "  'data/processed/pills-02-25-02.csv',\n",
       "  'data/processed/pills-02-25-03.csv',\n",
       "  'data/processed/pills-02-25-04.csv',\n",
       "  'data/processed/pills-02-25-05.csv',\n",
       "  'data/processed/pills-03-25-01.csv',\n",
       "  'data/processed/pills-03-25-02.csv',\n",
       "  'data/processed/pills-03-25-03.csv',\n",
       "  'data/processed/pills-03-25-04.csv',\n",
       "  'data/processed/pills-03-25-05.csv',\n",
       "  'data/processed/pills-04-25-01.csv',\n",
       "  'data/processed/pills-04-25-02.csv',\n",
       "  'data/processed/pills-04-25-03.csv',\n",
       "  'data/processed/pills-04-25-04.csv',\n",
       "  'data/processed/pills-04-25-05.csv',\n",
       "  'data/processed/pills-05-25-01.csv',\n",
       "  'data/processed/pills-05-25-02.csv',\n",
       "  'data/processed/pills-05-25-03.csv',\n",
       "  'data/processed/pills-05-25-04.csv',\n",
       "  'data/processed/pills-05-25-05.csv',\n",
       "  'data/processed/pills-06-25-01.csv',\n",
       "  'data/processed/pills-06-25-02.csv',\n",
       "  'data/processed/pills-06-25-03.csv',\n",
       "  'data/processed/pills-06-25-04.csv',\n",
       "  'data/processed/pills-06-25-05.csv',\n",
       "  'data/processed/pills-07-25-01.csv',\n",
       "  'data/processed/pills-07-25-02.csv',\n",
       "  'data/processed/pills-07-25-03.csv',\n",
       "  'data/processed/pills-07-25-04.csv',\n",
       "  'data/processed/pills-07-25-05.csv',\n",
       "  'data/processed/pills-08-25-01.csv',\n",
       "  'data/processed/pills-08-25-02.csv',\n",
       "  'data/processed/pills-08-25-03.csv',\n",
       "  'data/processed/pills-08-25-04.csv',\n",
       "  'data/processed/pills-08-25-05.csv',\n",
       "  'data/processed/pills-09-25-01.csv',\n",
       "  'data/processed/pills-09-25-02.csv',\n",
       "  'data/processed/pills-09-25-03.csv',\n",
       "  'data/processed/pills-09-25-04.csv',\n",
       "  'data/processed/pills-09-25-05.csv',\n",
       "  'data/processed/pills-10-25-01.csv',\n",
       "  'data/processed/pills-10-25-02.csv',\n",
       "  'data/processed/pills-10-25-03.csv',\n",
       "  'data/processed/pills-10-25-04.csv',\n",
       "  'data/processed/pills-10-25-05.csv',\n",
       "  'data/processed/pills-11-25-01.csv',\n",
       "  'data/processed/pills-11-25-02.csv',\n",
       "  'data/processed/pills-11-25-03.csv',\n",
       "  'data/processed/pills-11-25-04.csv',\n",
       "  'data/processed/pills-11-25-05.csv',\n",
       "  'data/processed/pills-12-25-01.csv',\n",
       "  'data/processed/pills-12-25-02.csv',\n",
       "  'data/processed/pills-12-25-03.csv',\n",
       "  'data/processed/pills-12-25-04.csv',\n",
       "  'data/processed/pills-12-25-05.csv',\n",
       "  'data/processed/pills-13-25-01.csv',\n",
       "  'data/processed/pills-13-25-02.csv',\n",
       "  'data/processed/pills-13-25-03.csv',\n",
       "  'data/processed/pills-13-25-04.csv',\n",
       "  'data/processed/pills-13-25-05.csv',\n",
       "  'data/processed/pills-14-25-01.csv',\n",
       "  'data/processed/pills-14-25-02.csv',\n",
       "  'data/processed/pills-14-25-03.csv',\n",
       "  'data/processed/pills-14-25-04.csv',\n",
       "  'data/processed/pills-14-25-05.csv',\n",
       "  'data/processed/pills-15-25-01.csv',\n",
       "  'data/processed/pills-15-25-02.csv',\n",
       "  'data/processed/pills-15-25-03.csv',\n",
       "  'data/processed/pills-15-25-04.csv',\n",
       "  'data/processed/pills-15-25-05.csv',\n",
       "  'data/processed/pills-16-25-01.csv',\n",
       "  'data/processed/pills-16-25-02.csv',\n",
       "  'data/processed/pills-16-25-03.csv',\n",
       "  'data/processed/pills-16-25-04.csv',\n",
       "  'data/processed/pills-16-25-05.csv',\n",
       "  'data/processed/pills-17-25-01.csv',\n",
       "  'data/processed/pills-17-25-02.csv',\n",
       "  'data/processed/pills-17-25-03.csv',\n",
       "  'data/processed/pills-17-25-04.csv',\n",
       "  'data/processed/pills-17-25-05.csv'],\n",
       " ['data/processed/pills-01-30-01.csv',\n",
       "  'data/processed/pills-01-30-02.csv',\n",
       "  'data/processed/pills-01-30-03.csv',\n",
       "  'data/processed/pills-01-30-04.csv',\n",
       "  'data/processed/pills-01-30-05.csv',\n",
       "  'data/processed/pills-02-30-01.csv',\n",
       "  'data/processed/pills-02-30-02.csv',\n",
       "  'data/processed/pills-02-30-03.csv',\n",
       "  'data/processed/pills-02-30-04.csv',\n",
       "  'data/processed/pills-02-30-05.csv',\n",
       "  'data/processed/pills-05-30-01.csv',\n",
       "  'data/processed/pills-05-30-02.csv',\n",
       "  'data/processed/pills-05-30-03.csv',\n",
       "  'data/processed/pills-05-30-04.csv',\n",
       "  'data/processed/pills-05-30-05.csv',\n",
       "  'data/processed/pills-06-30-01.csv',\n",
       "  'data/processed/pills-06-30-02.csv',\n",
       "  'data/processed/pills-06-30-03.csv',\n",
       "  'data/processed/pills-06-30-04.csv',\n",
       "  'data/processed/pills-06-30-05.csv',\n",
       "  'data/processed/pills-07-30-01.csv',\n",
       "  'data/processed/pills-07-30-02.csv',\n",
       "  'data/processed/pills-07-30-03.csv',\n",
       "  'data/processed/pills-07-30-04.csv',\n",
       "  'data/processed/pills-07-30-05.csv',\n",
       "  'data/processed/pills-08-30-01.csv',\n",
       "  'data/processed/pills-08-30-02.csv',\n",
       "  'data/processed/pills-08-30-03.csv',\n",
       "  'data/processed/pills-08-30-04.csv',\n",
       "  'data/processed/pills-08-30-05.csv',\n",
       "  'data/processed/pills-09-30-01.csv',\n",
       "  'data/processed/pills-09-30-02.csv',\n",
       "  'data/processed/pills-09-30-03.csv',\n",
       "  'data/processed/pills-09-30-04.csv',\n",
       "  'data/processed/pills-09-30-05.csv',\n",
       "  'data/processed/pills-10-30-01.csv',\n",
       "  'data/processed/pills-10-30-02.csv',\n",
       "  'data/processed/pills-10-30-03.csv',\n",
       "  'data/processed/pills-10-30-04.csv',\n",
       "  'data/processed/pills-10-30-05.csv',\n",
       "  'data/processed/pills-11-30-01.csv',\n",
       "  'data/processed/pills-11-30-02.csv',\n",
       "  'data/processed/pills-11-30-03.csv',\n",
       "  'data/processed/pills-11-30-04.csv',\n",
       "  'data/processed/pills-11-30-05.csv',\n",
       "  'data/processed/pills-12-30-01.csv',\n",
       "  'data/processed/pills-12-30-02.csv',\n",
       "  'data/processed/pills-12-30-03.csv',\n",
       "  'data/processed/pills-12-30-04.csv',\n",
       "  'data/processed/pills-12-30-05.csv',\n",
       "  'data/processed/pills-13-30-01.csv',\n",
       "  'data/processed/pills-13-30-02.csv',\n",
       "  'data/processed/pills-13-30-03.csv',\n",
       "  'data/processed/pills-13-30-04.csv',\n",
       "  'data/processed/pills-13-30-05.csv',\n",
       "  'data/processed/pills-14-30-01.csv',\n",
       "  'data/processed/pills-14-30-02.csv',\n",
       "  'data/processed/pills-14-30-03.csv',\n",
       "  'data/processed/pills-14-30-04.csv',\n",
       "  'data/processed/pills-14-30-05.csv',\n",
       "  'data/processed/pills-15-30-01.csv',\n",
       "  'data/processed/pills-15-30-02.csv',\n",
       "  'data/processed/pills-15-30-03.csv',\n",
       "  'data/processed/pills-15-30-04.csv',\n",
       "  'data/processed/pills-15-30-05.csv',\n",
       "  'data/processed/pills-16-30-01.csv',\n",
       "  'data/processed/pills-16-30-02.csv',\n",
       "  'data/processed/pills-16-30-03.csv',\n",
       "  'data/processed/pills-16-30-04.csv',\n",
       "  'data/processed/pills-16-30-05.csv',\n",
       "  'data/processed/pills-17-30-01.csv',\n",
       "  'data/processed/pills-17-30-02.csv',\n",
       "  'data/processed/pills-17-30-03.csv',\n",
       "  'data/processed/pills-17-30-04.csv',\n",
       "  'data/processed/pills-17-30-05.csv']]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Should be 8, one for each level of pill counts\n",
    "list(s3_paths.values())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "-----------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Kevin's Pre-processing Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-31T23:30:32.323609Z",
     "start_time": "2020-01-31T23:30:32.302632Z"
    },
    "code_folding": [
     61,
     92,
     99
    ]
   },
   "outputs": [],
   "source": [
    "def renameColumns(df):\n",
    "    \n",
    "    df_renamed = None\n",
    "    \n",
    "    try:\n",
    "        \n",
    "        df = df.select('loggingSample(N)', \n",
    "                        'gyroRotationX(rad/s)',\n",
    "                        'gyroRotationY(rad/s)',\n",
    "                        'gyroRotationZ(rad/s)',                       \n",
    "                        'avAudioRecorderPeakPower(dB)',\n",
    "                        'avAudioRecorderAveragePower(dB)')\n",
    "    except:\n",
    "        df = df.select('loggingSample', \n",
    "                       'gyroRotationX',\n",
    "                       'gyroRotationY',\n",
    "                       'gyroRotationZ', \n",
    "                       'avAudioRecorderPeakPower',\n",
    "                       'avAudioRecorderAveragePower')\n",
    "                        \n",
    "    renamed_df = df.toDF('loggingSample',\n",
    "                         'gyro_x',\n",
    "                         'gyro_y',\n",
    "                         'gyro_z',\n",
    "                         'audio_peak_power',\n",
    "                         'audio_average_power')\n",
    "    return renamed_df\n",
    "\n",
    "\n",
    "def trimData(df, trim_start, trim_end):\n",
    "    \n",
    "    count = df.count()\n",
    "    start_sample = math.ceil(count * trim_start)\n",
    "    end_sample = math.ceil(count * (1 - trim_end))\n",
    "\n",
    "    trimmed_df = df.where(f\"loggingSample > {start_sample} and loggingSample < {end_sample}\")\n",
    "    trimmed_df = trimmed_df.withColumn(\"loggingSampleAdjusted\", trimmed_df[\"loggingSample\"] - start_sample)\n",
    "    trimmed_df = trimmed_df.drop(\"loggingSample\").withColumnRenamed(\"loggingSampleAdjusted\",\"loggingSample\")\n",
    "    \n",
    "    return trimmed_df\n",
    "    \n",
    "def windowData(df, num_windows):\n",
    "    count = df.count()\n",
    "    df = df.where(f\"loggingSample < {count}\")\n",
    "    \n",
    "    window_size = count / num_windows\n",
    "    label_window = udf(lambda x : int(x // window_size), IntegerType())\n",
    "    \n",
    "    windowed_df = df.select(label_window('loggingSample'),\n",
    "                           'loggingSample',\n",
    "                           'gyro_x',\n",
    "                           'gyro_y',\n",
    "                           'gyro_z',\n",
    "                           'audio_peak_power',\n",
    "                           'audio_average_power'\n",
    "                          )\n",
    "    \n",
    "    windowed_df = windowed_df.withColumnRenamed(\"<lambda>(loggingSample)\", \"window\")\n",
    "    return windowed_df\n",
    "    \n",
    "    \n",
    "def transformData(df):\n",
    "    \n",
    "    df = df.withColumn(\"foo\", lit(\"foo\"))\n",
    "    transformed_df = df.groupBy(\"foo\").pivot(\"window\").agg(\n",
    "                                              round(min('gyro_x'),5),\n",
    "                                              round(max('gyro_x'),5), \n",
    "                                              round(avg('gyro_x'),5), \n",
    "                                              round(stddev('gyro_x'),5),\n",
    "                                              round(min('gyro_y'),5),\n",
    "                                              round(max('gyro_y'),5), \n",
    "                                              round(avg('gyro_y'),5), \n",
    "                                              round(stddev('gyro_y'),5),\n",
    "                                              round(min('gyro_z'),5),\n",
    "                                              round(max('gyro_z'),5), \n",
    "                                              round(avg('gyro_z'),5), \n",
    "                                              round(stddev('gyro_z'),5),\n",
    "                                              round(min('audio_peak_power'),5),\n",
    "                                              round(max('audio_peak_power'),5), \n",
    "                                              round(avg('audio_peak_power'),5), \n",
    "                                              round(stddev('audio_peak_power'),5),\n",
    "                                              round(min('audio_average_power'),5),\n",
    "                                              round(max('audio_average_power'),5), \n",
    "                                              round(avg('audio_average_power'),5), \n",
    "                                              round(stddev('audio_average_power'),5),\n",
    "                                             ).drop('foo')\n",
    "    \n",
    "    renamed_columns = [c.replace(\"(\",\"_\").replace(\")\",\"_\").replace(\"round_\",\"\")[:-5] for c in transformed_df.columns]\n",
    "    final_df = transformed_df.toDF(*renamed_columns)\n",
    "    \n",
    "    return final_df\n",
    "\n",
    "def appendMetaInfo(df, tdf):\n",
    "    \n",
    "    watchOnTwistHand, n_pills = df.select('watchOnTwistHand','n_pills').first()[:]\n",
    "    tdf = tdf.withColumn('watchOnTwistHand', lit(watchOnTwistHand)).withColumn('n_pills', lit(n_pills))\n",
    "    return tdf\n",
    "\n",
    "\n",
    "def preprocess(df, trim_start=0.05, trim_end=0.10, num_windows=10):\n",
    "        \n",
    "    # Select out and rename columns of interest\n",
    "    renamed_df = renameColumns(df)\n",
    "    \n",
    "    # Trim off ends of data\n",
    "    trimmed_df = trimData(renamed_df, trim_start, trim_end)\n",
    "\n",
    "    # Window data by labeling rows with window assignments \n",
    "    windowed_df = windowData(trimmed_df, num_windows)\n",
    "    \n",
    "    # Transform from long to wide with summarized statistics\n",
    "    transformed_df = transformData(windowed_df)\n",
    "    \n",
    "    # Append a feature and the data's label to the row\n",
    "    final_df = appendMetaInfo(df, transformed_df)\n",
    "    \n",
    "    return final_df.withColumnRenamed('n_pills', 'label')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Generate DFs in Mini-batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-31T23:30:32.331158Z",
     "start_time": "2020-01-31T23:30:32.326088Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Run on flattened list of file paths\n",
    "s3_paths = list(s3_paths.values())\n",
    "s3_paths = [x for path in s3_paths for x in path]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-31T23:30:32.342846Z",
     "start_time": "2020-01-31T23:30:32.337592Z"
    },
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "670"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Should be 670 in total\n",
    "len(s3_paths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-31T23:30:32.351506Z",
     "start_time": "2020-01-31T23:30:32.346290Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Need to append s3 prefix to read from bucket\n",
    "s3_prefix = \"s3://msds-twinkle/\"\n",
    "s3_paths = [s3_prefix + path for path in s3_paths]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-31T23:30:32.362164Z",
     "start_time": "2020-01-31T23:30:32.355802Z"
    },
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'s3://msds-twinkle/data/processed/pills-01-00-01.csv'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check for correct path\n",
    "s3_paths[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-31T23:30:53.614150Z",
     "start_time": "2020-01-31T23:30:39.482973Z"
    },
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    }
   ],
   "source": [
    "# Number of windows to use in pre-processing\n",
    "num_windows = 5\n",
    "\n",
    "# Initial path index & df\n",
    "start_idx = 0\n",
    "df_lite = ss.read.csv(s3_paths[start_idx], header=True, inferSchema=True)\n",
    "df_lite = preprocess(df_lite, num_windows=num_windows)\n",
    "\n",
    "# Adjust range to create smaller DFs, which we'll aggregate later\n",
    "for i in range(1):\n",
    "    print(i)  # DEBUG\n",
    "    df_i = ss.read.csv(s3_paths[i], header=True, inferSchema=True)\n",
    "    df_lite = df_lite.union(preprocess(df_i, num_windows=num_windows))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-31T23:30:57.170642Z",
     "start_time": "2020-01-31T23:30:57.155847Z"
    },
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "102"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check n_features\n",
    "len(df_lite.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "-------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transform Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Perform the following transformation on each mini-DF\n",
    "* Min-max transform each column\n",
    "* Shift each value by 0.001 (to miss 0)\n",
    "* Log-transform"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Write Mini-DFs to Disk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "len(df_lite.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# writes locally, this takes some time\n",
    "\n",
    "# Outpout path\n",
    "output_folder = \"~/DataScience/MSDS/sparkle/sparkle/data/training/\"\n",
    "filename = \"\"\n",
    "output_path - output_folder + filename\n",
    "\n",
    "df_lite.coalesce(1).write.option(\"header\", \"true\").csv(\"/Users/kevin/desktop/lite_window_10_2\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Join Mini-DFs to Get BigDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_0 = pd.read_csv('/users/kevin/desktop/lite_window_10_0/l0.csv')\n",
    "df_0.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_1 = pd.read_csv('/users/kevin/desktop/lite_window_10_0/l1.csv')\n",
    "df_1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_2 = pd.read_csv('/users/kevin/desktop/lite_window_10_0/l2.csv')\n",
    "df_2.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_0['label'] = (df_0['label'] <= 10).astype(int)\n",
    "big_df = df_0.copy()\n",
    "\n",
    "for df in [df_1, df_2]:\n",
    "    df['label'] = (df['label'] <= 10).astype(int)\n",
    "    print(df.label.value_counts())\n",
    "    big_df = big_df.append(df, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "big_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "big_df.label.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "big_df.to_csv('~/desktop/lite_629_w10.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.ml.classification import LogisticRegression, GBTClassifier, MultilayerPerceptronClassifier\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = '/users/kevin/desktop/lite_629_w10.csv'\n",
    "df_pills = ss.read.csv(data_path, header=True, inferSchema=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train-test split\n",
    "df_train, df_test = df_pills.randomSplit(weights=[0.80, 0.20])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_cols = df_train.drop(\"label\")\n",
    "\n",
    "# Transformer; excludes \"label\" col\n",
    "va = VectorAssembler(outputCol=\"features\", inputCols=df_train.columns[:-1])\n",
    "\n",
    "# Estimators\n",
    "gbt = GBTClassifier(maxIter=150, maxDepth=7, stepSize=0.3)\n",
    "lr = LogisticRegression(maxIter=2000, fitIntercept=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assemble features\n",
    "train_lpoints = va.transform(df_train).select(\"features\", \"label\").cache()\n",
    "test_lpoints = va.transform(df_test).select(\"features\", \"label\").cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_lpoints.groupBy(\"label\").count().show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train Gradient Boosted Tree Model\n",
    "start = time.time()\n",
    "gbt_model = gbt.fit(train_lpoints)\n",
    "print(time.time() - start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train Logistic Regression\n",
    "start = time.time()\n",
    "lr_model = lr.fit(train_lpoints)\n",
    "print(time.time() - start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict\n",
    "gbt_predict = gbt_model.transform(test_lpoints)\n",
    "lr_predict = lr_model.transform(test_lpoints)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics = MulticlassClassificationEvaluator().setLabelCol(\"label\").setPredictionCol(\"prediction\")\n",
    "\n",
    "metrics.setMetricName(\"accuracy\")\n",
    "print(f\"Gradient Boosted Accuracy: {metrics.evaluate(gbt_predict):.3f}\")\n",
    "print(f\"Logistic Reg Accuracy: {metrics.evaluate(lr_predict):.3f}\\n\")\n",
    "\n",
    "metrics.setMetricName(\"f1\")\n",
    "print(f\"Gradient Boosted F1: {metrics.evaluate(gbt_predict):.3f}\")\n",
    "print(f\"Logistic Reg F1: {metrics.evaluate(lr_predict):.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "toc": true
   },
   "source": [
    "<h1>Table of Contents<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#Imports-&amp;-Env-Setup\" data-toc-modified-id=\"Imports-&amp;-Env-Setup-1\">Imports &amp; Env Setup</a></span></li><li><span><a href=\"#Import-Collin's-Function-for-Grabbing-Data-From-s3\" data-toc-modified-id=\"Import-Collin's-Function-for-Grabbing-Data-From-s3-2\">Import Collin's Function for Grabbing Data From s3</a></span></li><li><span><a href=\"#Kevin's-Pre-processing-Code\" data-toc-modified-id=\"Kevin's-Pre-processing-Code-3\">Kevin's Pre-processing Code</a></span></li></ul></div>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
